---
title: "Snippets"
author: "Anne"
date: "8-1-2020"
output: html_document
---
### tryCatch
```{r setup, include=FALSE}
install.packages("tidyverse")
knitr::opts_chunk$set(echo = TRUE)
inputs = list(1, 2, 4, -5, 'oops', 0, 10)
for(input in inputs) {
    tryCatch(print(paste("log of", input, "=", log(input))),
             warning = function(w) 
               {print(paste("negative argument", input)); 
                                      log(-input)},
             error = function(e) 
               {print(paste("non-numeric argument", input));
                               NaN})
}

(r <- seq_along(1:3))
  
library(nycflights13)
library(tidyverse)

```
<div style="margin-bottom:40px;">
</div>
# R for Data Science 
## by Hadley Wickham 
### Chapter 1: Data visualization with ggplot2
```{r include = T}
## p. 6
# 3. 
?mpg

# 4. Scatterplot of hwy versus cyl
ggplot(mpg, aes(x = hwy, y = cyl)) +
  geom_point()

## p.12 

# 1.
ggplot(data = mpg) +
  geom_point(
    mapping = aes(x = displ, y = displ), color = "pink"
  )

# 2. see class of each variable with glimpse()

# 5. What does stroke do
ggplot(mtcars, aes(wt, mpg)) +
  geom_point(shape = 21, colour = "black", fill = "white", size = 2, stroke = 4)

# 6. Map aesthetic to sth other than a variable name 
ggplot(mpg, aes(x = displ, y = hwy, colour = displ < 6)) +
  geom_point()

  
```

### Chapter 3: Data transformation with dplyr 
* *filter* - choose certain variables only, like filter(flights, month >= 7, month <= 9)
  + with %in%: filter(flights, month %in% 7:9)
  + with booleans: filter(flights, dest == "IAH" | dest == "HOU")
  + with between: filter(flights, between(month, 7, 9))
  + with is.na: filter(flights, is.na(dep_time))
* *arrange* - sort values 
  + sorting in ascending order: arrange(flights, dep_delay)
  + to put NA first: arrange(flights, desc(is.na(dep_time)), dep_time)
* *mutate* -  calculate new variables and add them
  + like fastest_flights <- mutate(flights, mph = distance / air_time * 60)
  + transmute() > keep only new variables 
  + flights_deptime <-
  mutate(flights,
    dep_time_min = (dep_time %/% 100 * 60 + dep_time %% 100) %% 1440,
    sched_dep_time_min = (sched_dep_time %/% 100 * 60 +
      sched_dep_time %% 100) %% 1440,
    dep_delay_diff = dep_delay - dep_time_min + sched_dep_time_min
  )
* *select* - select columns by name
  + select by variable name: fastest_flights <- select(
  fastest_flights, mph, distance, air_time,
  flight, origin, dest, year, month, day
)
  + select by column names as strings: select(flights, "dep_time", "dep_delay", "arr_time", "arr_delay")
  + select by column numbers: select(flights, 4, 6, 7, 9)
  + Specify the names of the variables with character vector and one_of(): 
    + variables <- c("dep_time", "dep_delay", "arr_time", "arr_delay")
    + select(flights, one_of(variables))
  + starts_with: select(flights, starts_with("dep_"), starts_with("arr_"))
  + matches: select(flights, matches("^(dep|arr)_(time|delay)$"))
  + contains: select(flights, contains("_time"), contains("arr_"))
  + case-sensitive selection: ignore.case = FALSE
** *ranking*
  + row_number(): equivalent to row index 
  + min_rank(): assigns a rank equal to the number of values less than that tied value plus one
  + dense_rank(): assigns a rank equal to the number of distinct values less than that tied value plus one
  
* *trigonometry* 
  ++ sin, cos, tan, pi (see package Trig)

``` {r include = T, echo = T}

TRUE | FALSE
FALSE | NA

nycflights13::flights

## a) flights with a delay of two hours or more
(delay2hours <- filter(flights, arr_delay >= 120))

## b) flew to Houston 
hou <- (houst <- filter(flights, dest %in% c("IAH", "HOU")))

## g) departed between midnight and 6am
head(flights$time_hour)

## Logarithms can be practical 
log(99999)
log(9)

```

#### *group_by()* 
* creates grouping variable
* to use variable normally afterward, call ungroup()

#### *lag and lead*: 
```{r}
x <- c(1,1,2,2,3,4,5,6,6,7,8,9,9)
lag(x)
lead(x)
x != lag(x)

if (require("nycflights13")) {
carriers <- group_by(flights, carrier)
summarise(carriers, n())
mutate(carriers, n = n())
filter(carriers, n() < 100)
}

carriers <- group_by(flights, carrier) 
```

Convert dep_time and sched_dep_time to more convenient representation of number of minutes since midnight 
```{r}
head(flights$dep_time)
head(flights$sched_dep_time)

10 %% 2.4
```

#### *counts*
* whenever you aggregate data, use a count to make sure that you are not making conclusions based on very small amounts of data 

```{r}
flights <- nycflights13::flights
names(flights)
library(dplyr)
not_cancelled <- flights %>%
  filter(!is.na(dep_delay), !is.na(arr_delay))

cancelled <- flights %>% 
  filter(is.na(dep_delay), is.na(arr_delay))

unsure <- flights %>% 
  filter(is.na(dep_delay), is.na(arr_delay))

names(not_cancelled)
delays <- not_cancelled %>%
  group_by(month, flight, minute) %>% 
  summarize(
    delay = mean(arr_delay), delay_sd = sd(arr_delay),
    n = n()
  )

ggplot(data = delays, mapping = aes(x = n, y = delay)) + 
  geom_point(alpha = 1/10)


delays %>% 
  filter(n > 2) %>% 
  ggplot(mapping = aes(x = n, y = delay)) +
  geom_point(alpha = 1/4)
```

#### *progressively rolling up summaries*
```{r}
daily <- group_by(flights, year, month, day)
(per_day <- dplyr::summarize(daily, count = n()))
(per_month <- dplyr::summarize(per_day, count = n()))
(per_month <- dplyr::summarize(per_day, count = n()))
(per_month <- dplyr::summarize(per_month, count = n()))
```

#### *ungrouping* 
* if grouping needs to be removed, use ungroup()

```{r}
daily %>% 
  ungroup() %>%  # no longer grouped by date 
  dplyr::summarize(flights = n())
```
#### *arrange* - out NA first

```{r}
arrange(flights, desc(is.na(dep_time)), dep_time) ## this works since TRUE > FALSE (1 > 0)
```

#### *mutate* - sort to find the fastest flights 

```{r}
(fastest_flights <- mutate(flights, mph = distance / air_time * 60))
fastest_flights <- select(
  fastest_flights, mph, distance, air_time,
  flight, origin, dest, year, month, day
)
head(arrange(fastest_flights, desc(mph)))

```

### Exercises
```{r include = T, echo = T}
desti <- dplyr::group_by(flights, dep_delay, arr_delay, origin)
(per_desti <- dplyr::summarize(desti, mean = mean(dep_delay)))

# how can you specify the following in a different way?: 
(not_cancelled %>% count(dest)) 
not_cancelled %>% count(tailnum, wt = distance) # wt denotes column for which should be summed 

nrow(not_cancelled %>% count(tailnum, wt = distance))
?count
names(not_cancelled)
(dest <- group_by(not_cancelled, dest)) 
(summi <-  dplyr::summarize(dest, n = n()))

(taili <- group_by(not_cancelled, tailnum))
(summi <- dplyr::summarise(taili, n = sum(distance)))

## tally: Any arguments to tally() are summed: 
not_cancelled %>%
  group_by(tailnum) %>%
  tally(distance)

## 5.6.4 
# cancelled flights 
cancelled <- flights %>% 
  filter(is.na(dep_delay), is.na(arr_delay))

names(cancelled)
cancelled %>% 
  group_by(day, arr_delay) %>%
  summarise(n = n())

?summarise

(cancelled_per_day <-
  flights %>%
  mutate(cancelled = (is.na(arr_delay) | is.na(dep_delay))) %>%
  group_by(year, month, day) %>%
  summarise(
    cancelled_num = sum(cancelled),
    flights_num = n(),
  ))

ggplot(cancelled_per_day) +
  geom_point(aes(x = flights_num, y = cancelled_num))

names(cancelled_per_day)

(cancelled_and_delays <-
  flights %>%
  mutate(cancelled = (is.na(arr_delay) | is.na(dep_delay))) %>%
  group_by(year, month, day) %>%
  summarise(
    cancelled_prop = mean(cancelled),
    delay_prop = mean(dep_delay, na.rm = T),
    sth = mean(distance, na.rm = T),
  )) %>%
  ungroup()

ggplot(cancelled_and_delays) +
  geom_point(aes(x = delay_prop, y = cancelled_prop))

names(cancelled_and_delays)
(cancelled_and_delays_month <-
  cancelled_and_delays %>%
  group_by(year, month) %>%
  summarise(
    cancelled_pr = sum(cancelled_prop)
  )) %>%
  ungroup()

## 5.6.5 Which carrier has the worst delays? 
names(flights)
(flights %>% 
  group_by(carrier) %>% 
  summarise(depdel = mean(dep_delay, na.rm = T),
            arrdel = mean(arr_delay, na.rm = T)) %>% 
    arrange(desc(depdel))
  )
(flights %>% 
    group_by(carrier, dest) %>% 
    summarise(n()))

## disentangle effects of bad carriers and bad airports 
library(dplyr)
names(flights)
(bam <- (flights %>%
  filter(!is.na(arr_delay)) %>%
  # Total delay by carrier within each origin, dest
  group_by(origin, dest, carrier) %>%
  summarise(
    arr_delay = sum(arr_delay),
    flights = n()
  )))
# Total delay within each origin dest
tam <-  group_by(bam, origin, dest) %>%
  mutate(
    arr_delay_total = sum(arr_delay),
    flights_total = sum(flights))
# average delay of each carrier - average delay of other carriers
tam %>%
  mutate(
    arr_delay_others = (arr_delay_total - arr_delay) /
      (flights_total - flights),
    arr_delay_mean = arr_delay / flights,
    arr_delay_diff = arr_delay_mean - arr_delay_others
  ) %>%
# remove NaN values (when there is only one carrier)
filter(is.finite(arr_delay_diff)) %>%
# average over all airports it flies to
group_by(carrier) %>%
  summarise(arr_delay_diff = mean(arr_delay_diff)) %>%
  arrange(desc(arr_delay_diff))

?is.finite
```
#### remove NaN with is.finite()

#### What does the sort argument count() do? 
* It sorts the results in order of n. You could use this anytime you would run count() followed by arrange()

#### Grouped Mutates (and filters)
* Grouping is most useful in conjunction with summarise(), but you can also use it with mutate() and 

For example: find the worst members in a group
```{r}
flights %>%
  group_by(year, month, day) %>%
  select(year, month, day, arr_delay, carrier) %>%
  filter(rank(desc(arr_delay)) < 10 )
```
  
find all groups bigger than a threshold 
```{r}
names(flights)
(popular_dest <- flights %>%
    group_by(dest) %>%
    filter(n() > 365) %>%
    summarize(mean = mean(arr_delay, na.rm =T))) 
head(arrange(popular_dest, desc(mean)))
      
names(flights)
(unpopular_dest <- flights %>%
group_by(dest) %>%
    filter(n() < 365) %>%
    summarize(mean = mean(arr_delay, na.rm =T))) 
head(arrange(unpopular_dest, desc(mean)))
```

#### *standardize* to compute per group metrics:

```{r}
(popular_dest <- flights %>% 
    filter(arr_delay > 0) %>%
    mutate(prop_delay = arr_delay/ sum(arr_delay)) %>%
    select(year:day, dest, arr_delay, prop_delay))
```

      
#### a *grouped filter* is a grouped mutate followed by an ungrouped filter 
* functions that work most naturally in grouped mutates and filters are called 
* *window functions* (versus summary functions for summaries)
vignette("window-functions")

#### How do mutate and filtering functions change when combined with grouping?
* Summary functions (mean()), offset functions (lead(), lag()), ranking functions (min_rank(), row_number()), operate within each group when used with group_by() in mutate() or filter(). Arithmetic operators (+, -), logical operators (<, ==), modular arithmetic operators (%%, %/%), logarithmic functions (log) are not affected by group_by.
\n
* Summary functions like mean(), median(), sum(), std() and others covered in the section "Useful Summary Functions" calculate their values within each group when used with mutate() or filter() and group_by().

#### different means, depending on whether overall mean is called or group mean (by group_by)
```{r include = T, echo=T}
(tibble(
  x = 1:9,
  group = rep(c("a", "b", "c"), each = 3)
) %>%
  mutate(x_mean = mean(x)) %>%
  group_by(group) %>%
  mutate(x_mean_2 = mean(x)))
```
#### Arithmetic operators are not affected by group_by()
```{r}
(tibble(
  x = 1:9,
  group = rep(c("a", "b", "c"), each = 3)
) %>%
  mutate(y = x + 2) %>%
  group_by(group) %>%
  mutate(z = x + 2))
```

#### The modular arithmetic operators %/% and %% are not affected by group_by()
```{r}
(tibble(
  x = 1:9,
  group = rep(c("a", "b", "c"), each = 3)
) %>%
  mutate(y = x %% 2) %>%
  group_by(group) %>%
  mutate(z = x %% 2))
```

#### The logarithmic functions log(), log2(), and log10() are not affected by group_by().
```{r}
(tibble(
  x = 1:9,
  group = rep(c("a", "b", "c"), each = 3)
) %>%
  mutate(y = log(x)) %>%
  group_by(group) %>%
  mutate(z = log(x)))
```

#### The offset functions lead() and lag() respect the groupings in group_by(). The functions lag() and lead() will only return values within each group.
```{r}
(tibble(
  x = 1:9,
  group = rep(c("a", "b", "c"), each = 3)
) %>%
  mutate(lag_a = lag(x)) %>%
  group_by(group) %>%
  mutate(
    lag_x = lag(x),
    lead_x = lead(x))
  )
```

#### The cumulative and rolling aggregate functions cumsum(), cumprod(), cummin(), cummax(), and cummean() calculate values within each group.
```{r}
(tibble(
  x = 1:9,
  group = rep(c("a", "b", "c"), each = 3)
) %>%
  mutate(x_cumsum = cumsum(x)) %>%
  group_by(group) %>%
  mutate(x_cumsum_2 = cumsum(x)))
```

#### Logical comparisons, <, <=, >, >=, !=, and == are not affected by group_by().

```{r}
(tibble(
  x = 1:9,
  y = 9:1,
  group = rep(c("a", "b", "c"), each = 3)
) %>%
  mutate(x_lte_y = x <= y) %>%
  group_by(group) %>%
  mutate(x_lte_y_2 = x <= y))
```

#### Ranking functions like min_rank() work within each group when used with group_by().
```{r}
(tibble(
  x = 1:9,
  group = rep(c("a", "b", "c"), each = 3)
) %>%
  mutate(rnk = min_rank(x)) %>%
  group_by(group) %>%
  mutate(rnk2 = min_rank(x)))
```

####  note that arrange() ignores groups when sorting values.

```{r}
(tibble(
  x = runif(9),
  group = rep(c("a", "b", "c"), each = 3)
) %>%
  group_by(group) %>%
  arrange(x))
```

#### However, the order of values from arrange() can interact with groups when used with functions that rely on the ordering of elements, such as lead(), lag(), or cumsum().

```{r}
(tibble(
  group = rep(c("a", "b", "c"), each = 3),
  x = runif(9)
) %>%
  mutate(lag_xno = lag(x)) %>%
  group_by(group) %>%
  arrange(group) %>%
  mutate(lag_x = lag(x)) %>%
  mutate(lead_x = lead(x)))
```

#### min_rank exercise 
```{r}
mat <- matrix(1:9, nrow = 3, ncol = 3)
mat <- cbind(mat, c(8,2,12), c(2,3,1))
mat
min_rank(mat[5])
min_rank(mat[3])
?arrange
?min_rank

```

Which plane (tailnum) has the work on time record? 
```{r}
on_time <- flights %>%
  select(tailnum, arr_delay) %>%
  group_by(tailnum) %>%
  summarise(mean_delay = mean(arr_delay), n= n()) %>%
  filter(n >= 20) %>%
  arrange(desc(mean_delay))
```

The plane which few at least 20 flights with the worst on time record is:
```{r}
flights %>%
  filter(!is.na(tailnum)) %>%
  mutate(on_time = !is.na(arr_time) & (arr_delay <= 0)) %>%
  group_by(tailnum) %>%
  summarise(on_time = mean(on_time), n = n()) %>%
  filter(n >= 20) %>%
  arrange(on_time)

```

What time of the day should you fly when you want to avoid delays as much as possible? 
```{r}
flights %>%
  group_by(hour) %>%
  summarise(arr_delay = mean(arr_delay, na.rm = TRUE)) %>%
  arrange(arr_delay)
```

For each destination, compute the total minutes of delay. For each flight, compute the proportion of the total delay for its destination.
```{r}
flights %>%
  filter(arr_delay > 0) %>%
  group_by(dest) %>%
  mutate(total_delay = sum(arr_delay),
  arr_delay_prop = arr_delay / total_delay) %>%
  select(dest, total_delay, arr_delay_prop) %>%
  arrange(dest, desc(arr_delay_prop))
```

Delays are typically temporally correlated: even once the problem that caused the initial delay has been resolved, later flights are delayed to allow earlier flights to leave. Use lag() to explore how the delay of a flight is related to the delay of the immediately preceding flight.
```{r}
lagged <- flights %>%
  mutate(prev_delay = lag(dep_delay)) %>%
  filter(!is.na(dep_delay), !is.na(prev_delay))

lagged_delays <- lagged %>% 
  select(prev_delay, dep_delay, origin) %>%
  group_by(dep_delay, prev_delay, origin) %>%
  summarise(dep_delay_mean = mean(dep_delay))
lagged_delays

lagged_delays %>%
  group_by(prev_delay) %>%
  summarise(dep_delay_mean = mean(dep_delay)) %>%
  ggplot(aes(y = dep_delay_mean, x = prev_delay)) +
  geom_point() +
  scale_x_continuous(breaks = seq(0, 1500, by = 120)) +
  labs(y = "Departure Delay", x = "Previous Departure Delay")
```

The overall relationship looks similar in all three origin airports.
```{r}
lagged_delays %>%
  group_by(origin, prev_delay) %>%
  summarise(dep_delay_mean = mean(dep_delay)) %>%
  ggplot(aes(y = dep_delay_mean, x = prev_delay)) +
  geom_point() +
  facet_wrap(~origin, ncol = 2) +
  labs(y = "Departure Delay", x = "Previous Departure Delay")

```

Look at each destination. Can you find flights that are suspiciously fast? (i.e. flights that represent a potential data entry error). Compute the air time of a flight relative to the shortest flight to that destination. Which flights were most delayed in the air?
```{r}
names(flights)
flights %>%
  filter(!is.na(air_time)) %>%
  group_by(dest, origin) %>%
  summarise(mean_air = mean(air_time)) %>%
  arrange(mean_air)
```

To find unusual observations, we need to first put them on the same scale. I will standardize values by subtracting the mean from each and then dividing each by the standard deviation. \[ \mathsf{standardized}(x) = \frac{x - \mathsf{mean}(x)}{\mathsf{sd}(x)} . \] A standardized variable is often called a \(z\)-score. The units of the standardized variable are standard deviations from the mean. This will put the flight times from different routes on the same scale. The larger the magnitude of the standardized variable for an observation, the more unusual the observation is. Flights with negative values of the standardized variable are faster than the mean flight for that route, while those with positive values are slower than the mean flight for that route.

```{r}
standardized_flights <- flights %>%
  filter(!is.na(air_time)) %>%
  group_by(dest, origin) %>%
  mutate(
    air_time_mean = mean(air_time),
    air_time_sd = sd(air_time),
    n = n()
  ) %>%
  ungroup() %>%
  mutate(air_time_standard = (air_time - air_time_mean) / (air_time_sd + 1)) # 1 is added to avoid deviding by zero 

ggplot(standardized_flights, aes(x = air_time_standard)) +
  geom_density()
```

unusually fast flights = smallest standardized values 
```{r}
standardized_flights %>%
  arrange(air_time_standard) %>%
  select(
    carrier, flight, origin, dest, month, air_time, air_time_mean, air_time_standard
  ) %>%
  head(10) 
```
A potential issue with the way that we standardized the flights is that the mean and standard deviation used to calculate are sensitive to outliers and outliers is what we are looking for. Instead of standardizing variables with the mean and variance, we could use the median as a measure of central tendency and the interquartile range (IQR) as a measure of spread. The median and IQR are more resistant to outliers than the mean and standard deviation. The following method uses the median and inter-quartile range, which are less sensitive to outliers.
```{r}
standardized_flights2 <- flights %>%
  filter(!is.na(air_time)) %>%
  group_by(dest, origin) %>%
  mutate(
    air_time_median = median(air_time),
    air_time_iqr = IQR(air_time),
    n = n(),
    air_time_standard = (air_time - air_time_median) / air_time_iqr
  )

ggplot(standardized_flights2, aes(x = air_time_standard)) +
  geom_density()

names(flights)
flights %>%
  filter(dep_delay > 0) %>%
  mutate(mph = distance / (air_time/60)) %>%
  ggplot(aes(y = mph, x = dep_delay)) +
  geom_point() +
  scale_x_continuous(breaks = seq(0, 1300, by = 60)) +
  labs(y = "MPH", x = "Dep Delay")

flights %>%
  filter(dep_delay > 0) %>%
  mutate(mph = distance / (air_time/60)) %>%
  select(dep_delay, mph, arr_delay) %>%
  arrange(dep_delay)
```

Find all destinations that are flown by at least two carriers. Use that information to rank the carriers.
```{r}
flights %>%
  # find all airports with > 1 carrier
  group_by(dest) %>%
  mutate(n_carriers = n_distinct(carrier)) %>%
  filter(n_carriers > 1) %>%
  # rank carriers by numer of destinations
  group_by(carrier) %>%
  dplyr::summarize(n_dest = n_distinct(dest)) %>%
  arrange(desc(n_dest))
```

For each plane, count the number of flights before the first delay of greater than 1 hour.
```{r}
library(dplyr)
flights %>%
  # sort in increasing order
  select(tailnum, year, month, day, dep_delay) %>%
  filter(!is.na(dep_delay)) %>%
  arrange(tailnum, year, month, day) %>%
  group_by(tailnum) %>%
  # cumulative number of flights delayed over one hour
  mutate(cumulative_hr_delays = cumsum(dep_delay > 60))  %>%
  arrange(cumulative_hr_delays) %>%
  # count the number of flights == 0
  add_count(cumulative_hr_delays < 1, sort = T) 

?sort
```

### Chapter 5:  Exploratory Data Analysis 
* The *coord_cartesian()* function zooms in on the area specified by the limits, after having calculated and drawn the geoms. Since the histogram bins have already been calculated, it is unaffected
* *density* = count standardized 
* *Frequency Plot*: geom_freqpoly (count):
  + is good for lookup 
  + overlapping lines make it difficult to distinguish how overall distributions relate to each other 
* *violin plots*: geom_violin 
  + good for seeing overall shape of the distributions (skewness, central values, variance etc.)
  + difficult to see which category has the highest density for a given price 
* *Count plot*: geom_bar (shows count, x = categorical)
* *Histogram*: geom_histogram (shows count, x = continous)
  + good for seeing overall shape of the distributions (skewness, central values, variance etc.)
  + difficult to see which category has the highest density for a given price 
* *Boxplot*: geom_boxplot 
  + shows IQR (25-75)
  + good for seeing outliers 
  + x = categorical, y - continous 
  + good for ordinal (ordered) data; if not ordered, you can add order by using reorder()
* *Letter value plot*: One problem with box plots is that they were developed in an era of much smaller datasets and tend to display a prohibitively large number of “outlying values”. One approach to remedy this problem is the letter value plot.
  + Like box-plots, the boxes of the letter-value plot correspond to quantiles. However, they incorporate far more quantiles than box-plots. They are useful for larger datasets because,
    + larger datasets can give precise estimates of quantiles beyond the quartiles, and
    + in expectation, larger datasets should have more outliers (in absolute numbers).
* *Scatterplots*: if points begin to overplot (in very large data sets), use: alpha aesthetic to add transparency 
  + or even better: use geom_bin2d() and geom_hex() to bin two dimensions 
* to explore *subtleties*: remove the strong relationship between two variables and see what remains; for example by plotting the residuals 
``` {r include = T, echo - T}
ggplot(data = mpg, mapping = aes(x = class, y = hwy)) + 
  geom_boxplot()
```
#### reordering boxplot (so it's more informative)
```{r}
ggplot(data = mpg) + 
  geom_boxplot(mapping = aes(
    x = reorder (class, hwy, FUN = median),
    y = hwy
  )
)

```

#### flip the boxplot with coord_flip():
```{r}
ggplot(data = mpg) +
  geom_boxplot(aes(
    x = reorder(class, hwy, FUN = median),
    y = hwy
  )
) + 
coord_flip()
```

#### relationshio between price and carat

```{r}
ggplot(diamonds, aes(x = carat, y = price)) +
  geom_point()

ggplot(data = diamonds, mapping = aes(x = carat, y = price)) +
  geom_boxplot(mapping = aes(group = cut_width(carat, 0.1)))
```

#### The variables color and clarity are ordered categorical variables. The chapter suggests visualizing a categorical and continuous variable using *frequency polygons* or *boxplots*.
```{r}
diamonds %>%
  mutate(color = fct_rev(color)) %>% # I will reverse the order of the color levels so they will be in increasing order of quality on the x-axis. 
  ggplot(aes(x = color, y = price)) +
  geom_boxplot()

ggplot(data = diamonds) +
  geom_boxplot(mapping = aes(x = clarity, y = price))
```

For both clarity and color, there is a much larger amount of variation within each category than between categories. Carat is clearly the single best predictor of diamond prices.

Now that we have established that carat appears to be the best predictor of price, what is the relationship between it and cut? Since this is an example of a continuous (carat) and categorical (cut) variable, it can be visualized with a box plot.
```{r}
ggplot(diamonds, aes(x = cut, y = carat)) +
  geom_boxplot()
```
There is a lot of variability in the distribution of carat sizes within each cut category. There is a slight negative relationship between carat and cut. Noticeably, the largest carat diamonds have a cut of “Fair” (the lowest).

#### letter-value plot:
```{r}
library(lvplot)
ggplot(diamonds, aes(x = cut, y = price)) +
  geom_lv()
```

Larger datasets afford more precise estimates of *tail behavior*, but boxplots do not take advantage of this precision, instead presenting large numbers of extreme, though not unexpected, observations. Letter-value plots address this problem by including more detailed information about the tails using “letter values,” an order statistic defined by Tukey. Boxplots display the first two letter values (the median and quartiles); letter-value plots display further letter values so far as they are reliable estimates of their corresponding quantiles.
#### freq plot
``` {r include = T, echo - T}
ggplot(data = diamonds, mapping = aes(x = price, y = ..density..)) +
  geom_freqpoly(mapping = aes(color = cut), binwidth = 500)
```

#### Histogram
```{r}
ggplot(data = diamonds, mapping = aes(x = price)) +
  geom_histogram() +
  facet_wrap(~cut, ncol = 1, scales = "free_y")
```

#### Violin plot 
```{r}
ggplot(data = diamonds, mapping = aes(x = cut, y = price)) +
  geom_violin() +
  coord_flip()
```
There are two methods:
* *geom_quasirandom()* produces plots that are a mix of jitter and violin plots. There are several different methods that determine exactly how the random location of the points is generated.
* *geom_beeswarm()* produces a plot similar to a violin plot, but by offsetting the points.

How could you rescale the count dataset above to more clearly show the distribution of cut within color, or color within cut?
* To clearly show the distribution of cut within color, calculate a new variable prop which is the proportion of each cut within a color. This is done using a grouped mutate.

```{r}
library(viridis)
diamonds %>%
  count(color, cut) %>%
  group_by(color) %>%
  mutate(prop = n / sum(n)) %>%
  ggplot(mapping = aes(x = color, y = cut)) +
  geom_tile(mapping = aes(fill = prop)) +
  scale_fill_viridis(limits = c(0, 1))
```
I add limit = c(0, 1) to put the color scale between (0, 1). These are the logical boundaries of proportions. 

Use *geom_tile()* together with dplyr to explore how average flight delays vary by destination and month of year. What makes the plot difficult to read? How could you improve it?

```{r}
flights %>%
  group_by(month, dest) %>%
  summarise(dep_delay = mean(dep_delay, na.rm = TRUE)) %>%
  ggplot(aes(x = factor(month), y = dest, fill = dep_delay)) +
  geom_tile() +
  labs(x = "Month", y = "Destination", fill = "Departure Delay")

flights %>%
  group_by(month, dest) %>%
  summarise(dep_delay = mean(dep_delay, na.rm = TRUE)) %>%
  ggplot(aes(x = factor(month), y = dest, fill = dep_delay)) +
  geom_tile() +
  labs(x = "Month", y = "Destination", fill = "Departure Delay")
```

Better: 
```{r}
flights %>%
  group_by(month, dest) %>%
  summarise(dep_delay = mean(dep_delay, na.rm = TRUE)) %>%
  group_by(dest) %>%
  filter(n() == 12) %>% ##delete missing data (not flying in every month)
  ungroup() %>%
  mutate(dest = reorder(dest, dep_delay)) %>%
  ggplot(aes(x = factor(month), y = dest, fill = dep_delay)) +
  geom_tile() +
  scale_fill_viridis() +
  labs(x = "Month", y = "Destination", fill = "Departure Delay")

?reorder
```
#### Scatterplots
* use *alpha aesthetics* to add transparency 
```{r}
library(ggplot2)
ggplot(data = diamonds) + 
  geom_point(
    mapping = aes(x = carat, y = price),
    alpha = 1/100
  )
```

or, probably even better: use *geom_bin2d* or *geom_hex* plots:
```{r}
ggplot(data = diamonds) + 
  geom_bin2d(mapping = aes(x = carat, y = price))

library(hexbin)
ggplot(data = diamonds) + 
  geom_hex(mapping = aes(x = carat, y = price))
```

or treat one cont variable as a categorical variable and use *boxplot*:
```{r}
ggplot(data = diamonds, mapping = aes(x = carat, y = price)) +
  geom_boxplot(mapping = aes(group = cut_width(carat, 0.1)))
```
in order to see hdifference in number of observations, use: *varwidth = TRUE*
or use *cut_number* to display the same amount of points in each bin!

```{r}
ggplot(data = diamonds, mapping = aes(x = carat, y = price)) + 
  geom_boxplot(mapping = aes(group= cut_number(carat, 20)))
```

### Exercises
* using a freq plot instead of a conditional plot
* Both cut_width() and cut_number() split a variable into groups. When using cut_width(), we need to choose the width, and the number of bins will be calculated automatically. When using cut_number(), we need to specify the number of bins, and the widths will be calculated automatically.
* In either case, we want to choose the bin widths and number to be large enough to aggregate observations to remove noise, but not so large as to remove all the signal.
* If categorical colors are used, no more than eight colors should be used in order to keep them distinct. Using cut_number, I will split carats into quantiles (five groups).
```{r}
ggplot(
  data = diamonds,
  mapping = aes(color = cut_number(carat, 5), x = price)
) +
  geom_freqpoly() +
  labs(x = "Price", y = "Count", color = "Carat")
```

Alternatively, I could use cut_width to specify widths at which to cut. I will choose 1-carat widths. Since there are very few diamonds larger than 2-carats, this is not as informative. However, using a width of 0.5 carats creates too many groups, and splitting at non-whole numbers is unappealing.
```{r}
ggplot(
  data = diamonds,
  mapping = aes(color = cut_width(carat, 1, boundary = 0), x = price)
) +
  geom_freqpoly() +
  labs(x = "Price", y = "Count", color = "Carat")
```

Visualize the distribution of carat, partitioned by price.
```{r}
ggplot(diamonds, aes(x = cut_number(price, 10), y = carat)) +
  geom_boxplot() +
  coord_flip() +
  xlab("Price")
```

Plotted with a box plot with 10 equal-width bins of $2,000. The argument boundary = 0 ensures that first bin is $0–$2,000.
```{r}
ggplot(diamonds, aes(x = cut_width(price, 2000, boundary = 0), y = carat)) +
  geom_boxplot(varwidth = TRUE) +
  coord_flip() +
  xlab("Price")
```

Combine two of the techniques you’ve learned to visualize the combined distribution of cut, carat, and price.
```{r}
library(viridis)
ggplot(diamonds, aes(x = carat, y = price)) +
  geom_hex() +
  facet_wrap(~cut, ncol = 1) + # dusplays multiple plots as 'facets' of the cut variable 
  scale_fill_viridis()

ggplot(diamonds, aes(x = cut_number(carat, 5), y = price, colour = cut)) +
  geom_boxplot() +
  labs(x = "Carat", y = "Price")
```

In a normal scatterplot: 
```{r}
ggplot(data = diamonds) +
  geom_point(mapping = aes(x = x, y = y)) +
  coord_cartesian(xlim = c(4, 11), ylim = c(4, 11))
```

In this case, there is a strong relationship between x and y. The outliers in this case are not extreme in either x or y. A binned plot would not reveal these outliers, and may lead us to conclude that the largest value of x was an outlier even though it appears to fit the bivariate pattern well.

#### Plotting residuals: 
The residuals give a view of the price of the diamond, once the effect of carat is removed
*add_residuals* adds a single new column to a data frame:
```{r}
library(modelr)
mod <- lm(log(price) ~ log(carat), data = diamonds)
diamonds2 <- diamonds %>% 
  add_residuals(mod) %>%
  mutate(resid = exp(resid)) ## exp because log before 

ggplot(data = diamonds2) +
  geom_point(mapping = aes(x = carat, y = resid))

```
Once the strong relationship between carat and price is taken into account, you can see what you would expect in the relationhip between cut and price - relative to their size, better quality diamonds are more expensive:
```{r}
ggplot(data = diamonds2) + 
  geom_boxplot(mapping = aes(x = cut, y = resid))
```

### Relational Data - Exercises
I add the column flight_id as a surrogate key. I sort the data prior to making the key, even though it is not strictly necessary, so the order of the rows has some meaning.
```{r}
library(dplyr)
nycflights13::flights %>%
  arrange(year, month, day, sched_dep_time, carrier, flight) %>%
  mutate(flight_id = row_number()) %>%
  glimpse()


```
#### Check for primary key 
```{r}
Lahman::Batting %>%
  count(playerID, yearID, stint) %>%
  filter(n > 1) %>%
  nrow()
```
#### OmniGraffle to draw schemas! (See online)

#### package datamodelr to create schemas
```{r}
library(datamodelr)
dm1 <- dm_from_data_frames(list(
  Batting = Lahman::Batting,
  Master = Lahman::Master,
  Salaries = Lahman::Salaries
)) %>%
  dm_set_key("Batting", c("playerID", "yearID", "stint")) %>%
  dm_set_key("Master", "playerID") %>%
  dm_set_key("Salaries", c("yearID", "teamID", "playerID")) %>%
  dm_add_references(
    Batting$playerID == Master$playerID,
    Salaries$playerID == Master$playerID
  )

dm_create_graph(dm1, rankdir = "LR", columnArrows = TRUE) %>%
  dm_render_graph()
```
#### Anti join to check whether there are foreign key relations between tables 
```{r}
nrow(anti_join(Lahman::Pitching, Lahman::Batting,
  by = c("playerID", "yearID", "stint")
))
```

```{r}
sstr <- c("c","ab","B","bba","c",NA,"@","bla","a","Ba","%")
sstr[sstr %in% c(letters, LETTERS)]
setdiff(c(1:6,7:2),      c(3,7,12))

```
### Mutating joins 
```{r}
library(nycflights13)
library(tidyverse)
nycflights13::airports %>%
  semi_join(flights, c("faa" = "dest")) %>%
  ggplot(aes(lon, lat)) +
  borders("state") +
  geom_point() +
  coord_quickmap()
```
You might want to use the size or color of the points to display the average delay for each airport.
```{r}
avg_dest_delays <-
  flights %>%
  group_by(dest) %>%
  # arrival delay NA's are cancelled flights
  summarise(delay = mean(arr_delay, na.rm = TRUE)) %>%
  inner_join(airports, by = c(dest = "faa"))

avg_dest_delays %>%
  ggplot(aes(lon, lat, colour = delay)) +
  borders("state") +
  geom_point() +
  coord_quickmap()
```



 
